{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "import ast\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import json \n",
    "from requests import get\n",
    "import datetime\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import boto3\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')\n",
    "df = df[['headings','label']]\n",
    "map_ = {}\n",
    "c = 0\n",
    "for i in np.unique(df['label'].values):\n",
    "    map_[i] = c\n",
    "    c = c+1\n",
    "df['label_text'] = df['label']\n",
    "df['label'] =  df['label'].replace(map_)\n",
    "df.columns = ['text','label','label_text']\n",
    "df = df.sample(len(df[:10]))\n",
    "\n",
    "df['type_'] = ['train']*int(0.75*len(df)) + ['test']*(len(df) - int(0.75*len(df)))\n",
    "df_train = df[df['type_'] == 'train']\n",
    "del df_train['type_']\n",
    "df_test = df[df['type_'] == 'test']\n",
    "del df_test['type_']\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "test_ds = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=20,\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 280\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 18\n",
      "  Total train batch size = 16\n",
      "Iteration: 100%|██████████| 18/18 [01:07<00:00,  3.74s/it]\n",
      "Epoch: 100%|██████████| 1/1 [01:07<00:00, 67.39s/it]\n",
      "***** Running evaluation *****\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/kowshik/upsc-classification-model-v1 into local empty directory.\n",
      "Upload file pytorch_model.bin: 100%|█████████▉| 418M/418M [00:47<00:00, 11.3MB/s] remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/kowshik/upsc-classification-model-v1\n",
      "   6e57ece..32b02dd  main -> main\n",
      "\n",
      "Upload file pytorch_model.bin: 100%|██████████| 418M/418M [00:50<00:00, 8.73MB/s]\n",
      "Upload file model_head.pkl: 100%|██████████| 36.9k/36.9k [00:49<00:00, 102B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/kowshik/upsc-classification-model-v1/commit/32b02dde3071208aae8ffc6180a595ccac1d5696'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.push_to_hub('upsc-classification-model-v1',token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16105382, 0.16372876, 0.12344239, 0.15244631, 0.11823609,\n",
       "        0.28109262]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(['i am awesome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"kowshik/upsc-classification-model-v1\")\n",
    "predictions = model.predict(df_train.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5\n",
       "True     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(df_train['label'] == df_train['predictions']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3\n",
       "3     2\n",
       "0     1\n",
       "10    1\n",
       "Name: predictions, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     764\n",
       "11    761\n",
       "7     758\n",
       "10    757\n",
       "6     751\n",
       "4     751\n",
       "9     750\n",
       "1     742\n",
       "2     728\n",
       "12    590\n",
       "13    440\n",
       "0     421\n",
       "5     238\n",
       "8     168\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"data/upsc_pretrained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "from transformers import pipeline\n",
    "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(word,model_sim):\n",
    "    labels = ['Environment','History','Culture','Geography','International Relations',\n",
    "    'Polity','Governance','Health','Society','Economy','Science&Technology','Agriculture','sports']\n",
    "    labels = [i.lower() for i in labels]\n",
    "    embeddings_tags = model_sim.encode(labels)\n",
    "    embeddings_key = model_sim.encode(word)\n",
    "    probs = cosine_similarity([embeddings_key],embeddings_tags)\n",
    "    label_index = np.argmax(probs)\n",
    "    return(labels[label_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 59.0/59.0 [00:00<00:00, 42.4kB/s]\n",
      "Downloading: 100%|██████████| 829/829 [00:00<00:00, 666kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 229kB/s]  \n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.42kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 90.5kB/s]\n",
      "Downloading: 100%|██████████| 433M/433M [00:06<00:00, 67.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp2 = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp('inflation will affect private consumption but will moderate at the end of the projection period')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m(df\u001b[38;5;241m.\u001b[39mheadings\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m], get_label(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m],model_sim))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "for i in nlp(df.headings.values[4]):\n",
    "    print(i['word'], get_label(i['word'],model_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99259293,\n",
       "  'word': 'Goa Govt',\n",
       "  'start': 0,\n",
       "  'end': 8},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.98504794,\n",
       "  'word': 'Krishi Card',\n",
       "  'start': 19,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99259293,\n",
       "  'word': 'Goa Govt',\n",
       "  'start': 0,\n",
       "  'end': 8},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.98504794,\n",
       "  'word': 'Krishi Card',\n",
       "  'start': 19,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(df.headings.values[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Goa Govt launched 'Krishi Card' for farmers\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headings.values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('scrapeenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "050b2f7fdc8ceafbbec2485c71bd423c036445cf9e9b53c6874cc158d5607b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
